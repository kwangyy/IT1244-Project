{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note for KY - Jon and Zhiyong you can kind of get the context here too\n",
    "Once we get the dataset to be perfectly fine, lets test against different types of k-means clusters \n",
    "- Potential suggestion: Explore the dataset to see if there is any form of PCA needed\n",
    "\n",
    "Find the number of k with the lowest WSS, so that we're able to cluster them accordingly.\n",
    "After that, we can use the elbow method to find the best k value for the dataset and cluster them accordingly.\n",
    "\n",
    "After that, we manually analyse the data and from there we are able to then further group the data into two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = pd.read_cv(\"application.csv\")\n",
    "\n",
    "# Check if the data is balanced or not, if it is not we will have to do something to it\n",
    "app['insert_column_here'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = app.drop('insert_column_here', axis=1)\n",
    "y = app['insert_column_here']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model\n",
    "\n",
    "# We simplify the training of the model part\n",
    "def result(X_train, X_test, y_train, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg_result = result(X_train, X_test, y_train, log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we have the result for the few models we're using, we can optimise the hyperparameters\n",
    "# For example, we can use GridSearchCV to optimise the hyperparameters for Logistic Regression.\n",
    "# How do you find the parameters? Documentation\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# THIS IS AN EXAMPLE. ADD HOW YOU DEEM FIT.\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [100, 1000, 2500, 5000]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(log_reg, param_grid=param_grid)\n",
    "\n",
    "# After that, save this into a model and run result() again\n",
    "y_pred = result(X_train, X_test, y_train, clf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we use the ROC AUC score if there is an imbalance in dataset. We try not to use accuracy.\n",
    "\n",
    "There are also other scores like Precision and Recall, those are usable as well. \n",
    "\n",
    "However, let's look at the formulas for precision and recall:\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "The cost of having a false positive is higher than a false negative as we can deny the loan to a person who is not a defaulter, but we cannot give a loan to a person who is a defaulter.\n",
    "\n",
    "Therefore we should seek to minimise FP > minimise FN, and therefore the higher the precision, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Insert your model here\n",
    "model = None\n",
    "\n",
    "y_pred = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0,1,101)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    actual_positive = (y_test == 1)\n",
    "    actual_negative = (y_test == 0)\n",
    "\n",
    "    pred_positive = (y_pred >= t)\n",
    "    pred_negative = (y_pred < t)\n",
    "\n",
    "    tp = (actual_positive & pred_positive).sum()\n",
    "    tn = (actual_negative & pred_negative).sum()\n",
    "    fp = (pred_positive & actual_negative).sum()\n",
    "    fn = (pred_negative & actual_positive).sum()\n",
    "\n",
    "    scores.append((t, tp, fp, fn, tn))\n",
    "    \n",
    "columns = ['threshold', 'tp', 'fp', 'fn', 'tn']\n",
    "scores_data = pd.DataFrame(scores, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_data['tpr'] = scores_data.tp / (scores_data.tp + scores_data.fn)\n",
    "scores_data['fpr'] = scores_data.fp / (scores_data.fp + scores_data.tn)\n",
    "\n",
    "plt.plot(scores_data.threshold, scores_data.tpr, label = 'tpr')\n",
    "plt.plot(scores_data.threshold, scores_data.fpr, label = 'fpr')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make conclusions on the graph pls \n",
    "\n",
    "scores_data['precision'] = scores_data.tp / (scores_data.tp + scores_data.fp)\n",
    "scores_data['recall'] = scores_data.tp / (scores_data.tp + scores_data.fn)\n",
    "\n",
    "plt.plot(scores_data.threshold, scores_data.precision, label = 'precision')\n",
    "plt.plot(scores_data.threshold, scores_data.recall, label = 'recall')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
